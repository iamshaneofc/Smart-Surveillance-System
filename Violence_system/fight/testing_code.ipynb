{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9350703,"sourceType":"datasetVersion","datasetId":5668086},{"sourceId":9358925,"sourceType":"datasetVersion","datasetId":5668318},{"sourceId":116629,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":98026,"modelId":122210}],"dockerImageVersionId":30357,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div align=\"center\">\n\n  <a href=\"https://ultralytics.com/yolov8\" target=\"_blank\">\n    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\n\n\n<br>\n  <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>\n  <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n  <a href=\"https://www.kaggle.com/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n<br>\n\nWelcome to the Ultralytics YOLOv8 ðŸš€ notebook! <a href=\"https://github.com/ultralytics/ultralytics\">YOLOv8</a> is the latest version of the YOLO (You Only Look Once) AI models developed by <a href=\"https://ultralytics.com\">Ultralytics</a>. This notebook serves as the starting point for exploring the various resources available to help you get started with YOLOv8 and understand its features and capabilities.\n\nYOLOv8 models are fast, accurate, and easy to use, making them ideal for various object detection and image segmentation tasks. They can be trained on large datasets and run on diverse hardware platforms, from CPUs to GPUs.\n\nWe hope that the resources in this notebook will help you get the most out of YOLOv8. Please browse the YOLOv8 <a href=\"https://docs.ultralytics.com/\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/ultralytics\">GitHub</a> for support, and join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for questions and discussions!\n\n</div>","metadata":{"id":"t6MPjfT5NrKQ"}},{"cell_type":"markdown","source":"# Setup\n\nPip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) and check software and hardware.","metadata":{"id":"7mGmQbAO5pQb"}},{"cell_type":"code","source":"%pip install ultralytics\nimport ultralytics\nultralytics.checks()","metadata":{"id":"wbvMlHd_QwMG","outputId":"27ca383c-0a97-4679-f1c5-ba843f033de7","execution":{"iopub.status.busy":"2024-09-19T08:44:20.015524Z","iopub.execute_input":"2024-09-19T08:44:20.016746Z","iopub.status.idle":"2024-09-19T08:44:32.572847Z","shell.execute_reply.started":"2024-09-19T08:44:20.016704Z","shell.execute_reply":"2024-09-19T08:44:32.571609Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Ultralytics YOLOv8.0.145 ðŸš€ Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nSetup complete âœ… (4 CPUs, 31.4 GB RAM, 5846.1/8062.4 GB disk)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1. Predict\n\nYOLOv8 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLOv8 Predict Docs](https://docs.ultralytics.com/modes/train/).\n","metadata":{"id":"4JnkELT0cIJg"}},{"cell_type":"code","source":"# Run inference on an image with YOLOv8n\n!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'","metadata":{"id":"zR9ZbuQCH7FX","outputId":"64489d1f-e71a-44b5-92f6-2088781ca096"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Import the required libraries\nimport cv2\nfrom ultralytics import YOLO\n\n# Step 2: Load the YOLOv8 model (adjust path to your model)\nmodel = YOLO('/kaggle/input/yolov8-for-fight-detection/pytorch/default/1/best (6) (1).pt')  # Replace with your actual model path\n\n# Step 3: Function to detect class 1 (Violence) in a video and save the output\ndef detect_class_in_video(video_path, output_path):\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n\n    # Get video properties\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    # Define the codec and create VideoWriter to save the output video\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4 format\n    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n    # Process the video frames\n    while True:\n        ret, frame = cap.read()  # Read a frame from the video\n        if not ret:\n            break  # If no frame is returned, break the loop (end of video)\n\n        # Run YOLOv8 inference on the frame\n        results = model(frame)\n\n        # Extract the detection results (bounding boxes)\n        for detection in results[0].boxes:\n            class_id = int(detection.cls)  # Get the class ID of the detection\n            confidence = float(detection.conf)  # Get the confidence score of the detection\n\n            # Check if the detection is for class ID 1 (Violence)\n            if class_id == 1:  # Only process if it's 'Violence'\n                x1, y1, x2, y2 = map(int, detection.xyxy[0])  # Get the bounding box coordinates\n\n                # Draw the bounding box and label on the frame\n                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Red box for class 1\n                cv2.putText(frame, f'Violence {confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n\n        # Write the processed frame to the output video\n        out.write(frame)\n\n    # Release video resources\n    cap.release()\n    out.release()\n\n    print(f\"Processed video saved at {output_path}\")\n\n# Step 4: Test the function with a video file\nvideo_path = '/kaggle/input/test-video/fight video demo.mp4'  # Replace with your input video path\noutput_path = '/kaggle/working/tested_video3.mp4'  # Corrected output video path\ndetect_class_in_video(video_path, output_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:44:32.574816Z","iopub.execute_input":"2024-09-19T08:44:32.575181Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\n0: 384x640 1 non_violence, 18.1ms\nSpeed: 8.4ms preprocess, 18.1ms inference, 23.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.9ms\nSpeed: 2.9ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.0ms\nSpeed: 2.3ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 2.3ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.1ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.0ms preprocess, 9.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 12.2ms\nSpeed: 3.1ms preprocess, 12.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 2.2ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.9ms\nSpeed: 2.2ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.5ms\nSpeed: 2.2ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 8.9ms\nSpeed: 3.1ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.0ms\nSpeed: 2.1ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.4ms\nSpeed: 2.2ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.3ms\nSpeed: 2.1ms preprocess, 9.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.9ms\nSpeed: 3.0ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.1ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.3ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.6ms\nSpeed: 2.7ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.2ms\nSpeed: 3.0ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.8ms preprocess, 8.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.1ms\nSpeed: 3.0ms preprocess, 11.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 9.1ms\nSpeed: 2.8ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 8.9ms\nSpeed: 2.2ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.6ms\nSpeed: 3.2ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.2ms preprocess, 8.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.9ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.8ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.1ms\nSpeed: 3.1ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.2ms\nSpeed: 3.1ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 violences, 11.8ms\nSpeed: 3.0ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.2ms\nSpeed: 2.1ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 8.8ms\nSpeed: 2.3ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.9ms\nSpeed: 3.0ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.1ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.9ms\nSpeed: 2.9ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.7ms\nSpeed: 2.8ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.9ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.6ms\nSpeed: 3.0ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.4ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.8ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.8ms\nSpeed: 2.9ms preprocess, 11.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.3ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.1ms\nSpeed: 3.0ms preprocess, 10.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.9ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.8ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 3.1ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.9ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.7ms\nSpeed: 2.9ms preprocess, 9.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.2ms\nSpeed: 3.2ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.0ms\nSpeed: 3.0ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.4ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 12.2ms\nSpeed: 3.0ms preprocess, 12.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.0ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.7ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 16.0ms\nSpeed: 3.3ms preprocess, 16.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.1ms\nSpeed: 2.9ms preprocess, 9.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.8ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.3ms\nSpeed: 2.8ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.8ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.3ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 3.0ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.0ms\nSpeed: 2.9ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.8ms\nSpeed: 2.8ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.7ms preprocess, 8.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.2ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.8ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 3.0ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.2ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 3.0ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.7ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.9ms\nSpeed: 3.3ms preprocess, 10.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.7ms\nSpeed: 2.3ms preprocess, 9.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 3.1ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.9ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.9ms\nSpeed: 3.0ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.8ms preprocess, 8.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.5ms\nSpeed: 3.1ms preprocess, 11.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 2.2ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.9ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.9ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 3.1ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.3ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.2ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 3.1ms preprocess, 8.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.3ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.2ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.3ms\nSpeed: 3.2ms preprocess, 11.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.7ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 3.1ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 non_violences, 8.8ms\nSpeed: 2.8ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.2ms\nSpeed: 3.0ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.7ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 10.7ms\nSpeed: 2.9ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.1ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.1ms\nSpeed: 3.0ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.8ms\nSpeed: 3.1ms preprocess, 11.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.2ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.4ms\nSpeed: 3.2ms preprocess, 11.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.8ms\nSpeed: 2.2ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.4ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.2ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.4ms\nSpeed: 3.2ms preprocess, 11.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 2.8ms preprocess, 9.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.6ms\nSpeed: 2.2ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 10.7ms\nSpeed: 3.0ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 12.5ms\nSpeed: 3.0ms preprocess, 12.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 8.8ms\nSpeed: 3.1ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 11.7ms\nSpeed: 2.9ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 10.3ms\nSpeed: 3.0ms preprocess, 10.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 10.9ms\nSpeed: 3.2ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.5ms\nSpeed: 3.1ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.3ms\nSpeed: 2.9ms preprocess, 9.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.6ms\nSpeed: 2.9ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.8ms\nSpeed: 2.9ms preprocess, 11.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.3ms\nSpeed: 2.9ms preprocess, 9.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 10.8ms\nSpeed: 2.9ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.2ms\nSpeed: 2.9ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.7ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 8.6ms\nSpeed: 2.7ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.8ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.8ms\nSpeed: 3.1ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.1ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.5ms\nSpeed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.2ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.2ms\nSpeed: 3.0ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 9.1ms\nSpeed: 2.2ms preprocess, 9.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.3ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 3.1ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.3ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.4ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.9ms\nSpeed: 2.1ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.2ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.1ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.5ms\nSpeed: 2.3ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.9ms\nSpeed: 3.0ms preprocess, 11.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.3ms\nSpeed: 3.1ms preprocess, 11.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.0ms\nSpeed: 2.9ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.9ms preprocess, 9.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.2ms\nSpeed: 2.3ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.2ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.2ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.2ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.4ms\nSpeed: 2.3ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.9ms\nSpeed: 3.0ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.1ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.2ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.4ms\nSpeed: 2.3ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 10.9ms\nSpeed: 3.2ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.4ms\nSpeed: 2.2ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.2ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 8.8ms\nSpeed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 10.5ms\nSpeed: 3.1ms preprocess, 10.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.4ms\nSpeed: 2.1ms preprocess, 9.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.2ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.2ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.4ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.8ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.8ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.7ms\nSpeed: 3.0ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.0ms\nSpeed: 3.0ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.3ms\nSpeed: 3.1ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.9ms\nSpeed: 3.0ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.3ms\nSpeed: 3.0ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.2ms\nSpeed: 3.0ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.7ms\nSpeed: 3.1ms preprocess, 11.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.3ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 10.5ms\nSpeed: 3.1ms preprocess, 10.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 8.9ms\nSpeed: 2.9ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.7ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.8ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.9ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.4ms\nSpeed: 2.9ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.6ms\nSpeed: 2.7ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.0ms\nSpeed: 3.1ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.1ms\nSpeed: 2.8ms preprocess, 9.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.9ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 12.3ms\nSpeed: 3.2ms preprocess, 12.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.2ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.2ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.1ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.5ms\nSpeed: 3.2ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.2ms\nSpeed: 2.9ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 9.8ms\nSpeed: 2.3ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.1ms\nSpeed: 2.4ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.8ms\nSpeed: 3.0ms preprocess, 9.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 12.1ms\nSpeed: 3.0ms preprocess, 12.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.1ms\nSpeed: 2.2ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.1ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.3ms\nSpeed: 3.0ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.2ms\nSpeed: 2.2ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 2 violences, 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.1ms\nSpeed: 3.0ms preprocess, 9.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 11.4ms\nSpeed: 3.0ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.7ms\nSpeed: 2.1ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 10.4ms\nSpeed: 2.9ms preprocess, 10.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.9ms\nSpeed: 3.0ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.8ms\nSpeed: 3.0ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.3ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 12.0ms\nSpeed: 3.0ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.6ms\nSpeed: 2.8ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.4ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.1ms\nSpeed: 3.1ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.1ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 10.9ms\nSpeed: 3.1ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.1ms\nSpeed: 3.1ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.6ms\nSpeed: 2.7ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.5ms\nSpeed: 3.2ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.4ms\nSpeed: 3.0ms preprocess, 11.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.3ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.1ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.4ms\nSpeed: 2.8ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.9ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.1ms\nSpeed: 3.2ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 11.1ms\nSpeed: 3.1ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 non_violences, 1 violence, 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 10.8ms\nSpeed: 2.9ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.2ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.1ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.7ms\nSpeed: 2.2ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.1ms\nSpeed: 2.4ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 10.0ms\nSpeed: 3.1ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.2ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.1ms\nSpeed: 3.1ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.2ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.9ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 2.8ms preprocess, 9.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.0ms\nSpeed: 2.3ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 2.3ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 3.0ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.8ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.3ms\nSpeed: 3.2ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.8ms preprocess, 8.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.3ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 2.2ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.2ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.9ms preprocess, 8.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.8ms\nSpeed: 3.0ms preprocess, 9.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 3.1ms preprocess, 8.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.8ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 non_violences, 8.4ms\nSpeed: 2.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.8ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 10.6ms\nSpeed: 3.1ms preprocess, 10.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.8ms\nSpeed: 2.3ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.8ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.1ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.1ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.4ms\nSpeed: 2.3ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.2ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.9ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.9ms\nSpeed: 3.1ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.8ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.7ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.9ms preprocess, 9.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.9ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 3.0ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.8ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.8ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.9ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 2.1ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.7ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.2ms\nSpeed: 3.0ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.6ms\nSpeed: 3.0ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.2ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.2ms\nSpeed: 3.1ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.3ms\nSpeed: 3.0ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 2.4ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 3.0ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.4ms\nSpeed: 2.4ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 2.1ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 10.6ms\nSpeed: 2.3ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 10.2ms\nSpeed: 3.0ms preprocess, 10.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.0ms\nSpeed: 3.0ms preprocess, 10.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.4ms\nSpeed: 3.0ms preprocess, 10.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.9ms\nSpeed: 3.2ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 2.2ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.6ms\nSpeed: 3.0ms preprocess, 9.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.9ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.4ms\nSpeed: 3.0ms preprocess, 11.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.1ms\nSpeed: 3.1ms preprocess, 10.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 12.5ms\nSpeed: 3.3ms preprocess, 12.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.4ms\nSpeed: 3.1ms preprocess, 11.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.8ms\nSpeed: 3.2ms preprocess, 9.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.0ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.4ms\nSpeed: 3.0ms preprocess, 11.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.1ms\nSpeed: 3.1ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.9ms\nSpeed: 3.1ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.9ms\nSpeed: 2.2ms preprocess, 11.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.8ms\nSpeed: 3.3ms preprocess, 9.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 12.0ms\nSpeed: 3.0ms preprocess, 12.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 13.8ms\nSpeed: 3.0ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 12.9ms\nSpeed: 3.1ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.0ms\nSpeed: 2.7ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.5ms\nSpeed: 2.4ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.1ms\nSpeed: 2.2ms preprocess, 9.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 non_violences, 9.9ms\nSpeed: 2.2ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 11.1ms\nSpeed: 3.2ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 non_violences, 1 violence, 12.7ms\nSpeed: 3.2ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 non_violences, 1 violence, 9.5ms\nSpeed: 2.9ms preprocess, 9.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 11.4ms\nSpeed: 3.1ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 2 violences, 10.0ms\nSpeed: 3.1ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.4ms\nSpeed: 2.7ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.6ms\nSpeed: 3.0ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.9ms\nSpeed: 2.1ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 11.1ms\nSpeed: 3.0ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.0ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.4ms\nSpeed: 3.0ms preprocess, 9.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.4ms\nSpeed: 2.3ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 8.6ms\nSpeed: 2.0ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 violences, 10.7ms\nSpeed: 3.0ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.5ms\nSpeed: 2.8ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.5ms\nSpeed: 2.8ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.7ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 11.1ms\nSpeed: 3.1ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 non_violences, 8.8ms\nSpeed: 2.7ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.6ms\nSpeed: 2.1ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.6ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.5ms\nSpeed: 2.3ms preprocess, 9.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.4ms\nSpeed: 2.7ms preprocess, 9.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 10.9ms\nSpeed: 2.7ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.3ms\nSpeed: 2.9ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 2.9ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.3ms\nSpeed: 2.9ms preprocess, 9.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 3.0ms preprocess, 9.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.0ms\nSpeed: 2.7ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.2ms\nSpeed: 2.2ms preprocess, 9.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.5ms\nSpeed: 2.2ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.2ms preprocess, 9.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.9ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.3ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 2.5ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 2.1ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.3ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.1ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.3ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.6ms\nSpeed: 2.9ms preprocess, 9.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 3.0ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.9ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 3.0ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.6ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 2.7ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 2.2ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 2.2ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.1ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.1ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.4ms\nSpeed: 2.4ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 2.3ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.2ms\nSpeed: 2.2ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 11.6ms\nSpeed: 3.0ms preprocess, 11.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.2ms\nSpeed: 3.0ms preprocess, 10.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 12.5ms\nSpeed: 3.1ms preprocess, 12.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 12.1ms\nSpeed: 2.9ms preprocess, 12.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.7ms\nSpeed: 3.0ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.3ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.4ms preprocess, 9.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.2ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.2ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.7ms\nSpeed: 2.1ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.2ms\nSpeed: 2.8ms preprocess, 9.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.7ms\nSpeed: 2.7ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.1ms\nSpeed: 2.3ms preprocess, 9.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.3ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 2.3ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.3ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 9.5ms\nSpeed: 2.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.7ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.1ms\nSpeed: 2.7ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.3ms\nSpeed: 2.9ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.8ms\nSpeed: 2.9ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 9.3ms\nSpeed: 2.3ms preprocess, 9.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 non_violences, 8.7ms\nSpeed: 2.8ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 8.6ms\nSpeed: 2.7ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 9.0ms\nSpeed: 2.8ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 violence, 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.5ms\nSpeed: 2.7ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 non_violence, 1 violence, 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/212889447-69e5bdf1-5800-4e29-835e-2ed2336dede2.jpg\" width=\"600\">","metadata":{"id":"hkAzDWJ7cWTr"}},{"cell_type":"markdown","source":"# 2. Val\nValidate a model's accuracy on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset's `val` or `test` splits. The latest YOLOv8 [models](https://github.com/ultralytics/ultralytics#models) are downloaded automatically the first time they are used. See [YOLOv8 Val Docs](https://docs.ultralytics.com/modes/val/) for more information.","metadata":{"id":"0eq1SMWl6Sfn"}},{"cell_type":"code","source":"# Download COCO val\nimport torch\ntorch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip","metadata":{"id":"WQPtK1QYVaD_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validate YOLOv8n on COCO8 val\n!yolo val model=yolov8n.pt data=coco8.yaml","metadata":{"id":"X58w8JLpMnjH","outputId":"e3aacd98-ceca-49b7-e112-a0c25979ad6c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Train\n\n<p align=\"\"><a href=\"https://bit.ly/ultralytics_hub\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\"/></a></p>\n\nTrain YOLOv8 on [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/) datasets. See [YOLOv8 Train Docs](https://docs.ultralytics.com/modes/train/) for more information.","metadata":{"id":"ZY2VXXXu74w5"}},{"cell_type":"code","source":"# Train YOLOv8n on COCO8 for 3 epochs\n!yolo train model=yolov8n.pt data=coco8.yaml epochs=3 imgsz=640","metadata":{"id":"1NcFxRcFdJ_O","outputId":"b750f2fe-c4d9-4764-b8d5-ed7bd920697b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Export\n\nExport a YOLOv8 model to any supported format below with the `format` argument, i.e. `format=onnx`. See [YOLOv8 Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n\n- ðŸ’¡ ProTip: Export to [ONNX](https://onnx.ai/) or [OpenVINO](https://docs.openvino.ai/latest/index.html) for up to 3x CPU speedup.  \n- ðŸ’¡ ProTip: Export to [TensorRT](https://developer.nvidia.com/tensorrt) for up to 5x GPU speedup.\n\n\n| Format                                                             | `format` Argument | Model                     | Metadata | Arguments                                           |\n|--------------------------------------------------------------------|-------------------|---------------------------|----------|-----------------------------------------------------|\n| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | âœ…        | -                                                   |\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | âœ…        | `imgsz`, `optimize`                                 |\n| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `opset`     |\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | âœ…        | `imgsz`, `half`                                     |\n| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `workspace` |\n| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlmodel`         | âœ…        | `imgsz`, `half`, `int8`, `nms`                      |\n| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | âœ…        | `imgsz`, `keras`                                    |\n| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | âŒ        | `imgsz`                                             |\n| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | âœ…        | `imgsz`, `half`, `int8`                             |\n| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | âœ…        | `imgsz`                                             |\n| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | âœ…        | `imgsz`                                             |\n| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | âœ…        | `imgsz`                                             |\n| [ncnn](https://github.com/Tencent/ncnn)                            | `ncnn`            | `yolov8n_ncnn_model/`     | âœ…        | `imgsz`, `half`                                     |\n","metadata":{"id":"nPZZeNrLCQG6"}},{"cell_type":"code","source":"!yolo export model=yolov8n.pt format=torchscript","metadata":{"id":"CYIjW4igCjqD","outputId":"2b65e381-717b-4a6f-d6f5-5254c867f3a4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Python Usage\n\nYOLOv8 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLOv8 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLOv8 Python Docs](https://docs.ultralytics.com/usage/python/).","metadata":{"id":"kUMOQ0OeDBJG"}},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO('yolov8n.yaml')  # build a new model from scratch\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n\n# Use the model\nresults = model.train(data='coco128.yaml', epochs=3)  # train the model\nresults = model.val()  # evaluate model performance on the validation set\nresults = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\nresults = model.export(format='onnx')  # export the model to ONNX format","metadata":{"id":"bpF9-vS_DAaf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Tasks\n\nYOLOv8 can train, val, predict and export models for the most common tasks in vision AI: [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/). See [YOLOv8 Tasks Docs](https://docs.ultralytics.com/tasks/) for more information.\n\n<br><img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png\">\n","metadata":{"id":"Phm9ccmOKye5"}},{"cell_type":"markdown","source":"## 1. Detection\n\nYOLOv8 _detection_ models have no suffix and are the default YOLOv8 models, i.e. `yolov8n.pt` and are pretrained on COCO. See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for full details.\n","metadata":{"id":"yq26lwpYK1lq"}},{"cell_type":"code","source":"# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection model\nmodel.train(data='coco128.yaml', epochs=3)  # train the model\nmodel('https://ultralytics.com/images/bus.jpg')  # predict on an image","metadata":{"id":"8Go5qqS9LbC5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Segmentation\n\nYOLOv8 _segmentation_ models use the `-seg` suffix, i.e. `yolov8n-seg.pt` and are pretrained on COCO. See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for full details.\n","metadata":{"id":"7ZW58jUzK66B"}},{"cell_type":"code","source":"# Load YOLOv8n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n-seg.pt')  # load a pretrained YOLOv8n segmentation model\nmodel.train(data='coco128-seg.yaml', epochs=3)  # train the model\nmodel('https://ultralytics.com/images/bus.jpg')  # predict on an image","metadata":{"id":"WFPJIQl_L5HT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Classification\n\nYOLOv8 _classification_ models use the `-cls` suffix, i.e. `yolov8n-cls.pt` and are pretrained on ImageNet. See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for full details.\n","metadata":{"id":"ax3p94VNK9zR"}},{"cell_type":"code","source":"# Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\nmodel.train(data='mnist160', epochs=3)  # train the model\nmodel('https://ultralytics.com/images/bus.jpg')  # predict on an image","metadata":{"id":"5q9Zu6zlL5rS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Pose\n\nYOLOv8 _pose_ models use the `-pose` suffix, i.e. `yolov8n-pose.pt` and are pretrained on COCO Keypoints. See [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for full details.","metadata":{"id":"SpIaFLiO11TG"}},{"cell_type":"code","source":"# Load YOLOv8n-pose, train it on COCO8-pose for 3 epochs and predict an image with it\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n-pose.pt')  # load a pretrained YOLOv8n classification model\nmodel.train(data='coco8-pose.yaml', epochs=3)  # train the model\nmodel('https://ultralytics.com/images/bus.jpg')  # predict on an image","metadata":{"id":"si4aKFNg19vX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Appendix\n\nAdditional content below.","metadata":{"id":"IEijrePND_2I"}},{"cell_type":"code","source":"# Git clone and run tests on updates branch\n!git clone https://github.com/ultralytics/ultralytics -b main\n%pip install -qe ultralytics","metadata":{"id":"uRKlwxSJdhd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run tests (Git clone only)\n!pytest ultralytics/tests","metadata":{"id":"GtPlh7mcCGZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validate multiple models\nfor x in 'nsmlx':\n  !yolo val model=yolov8{x}.pt data=coco.yaml","metadata":{"id":"Wdc6t_bfzDDk"},"execution_count":null,"outputs":[]}]}